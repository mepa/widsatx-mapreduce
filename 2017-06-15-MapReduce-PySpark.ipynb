{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=images/widsatx-logo-outline.png align=\"left\" width=\"18%\">\n",
    "\n",
    "<h1 align='center'>MapReduce, etc.</h1>\n",
    "<h3 align='center'>June 15, 2017</h3>\n",
    "<h3 align='center'>Women in Data Science - ATX</h3>\n",
    "<h3 align='center'>Data Science from Scratch Workshop</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question: What sorts of computational blockers have you come across?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some Solutions\n",
    "#### CPU Bound\n",
    "    - Optimize algorithms\n",
    "    - Optimize compilation\n",
    "    - Parallelize code\n",
    "#### I/O Bound\n",
    "    - Decrease network transfer time\n",
    "        - Compress data for faster transfer\n",
    "        - Move computation to the data\n",
    "    - Consider multithreading\n",
    "        - Unblock code by allowing data to transfer in a separate thread\n",
    "#### Storage Bound\n",
    "    - Store data elsewhere and transfer to compute machine when needed\n",
    "    - Spread data across multiple machines and run computations on all machines\n",
    "#### Memory Bound\n",
    "    - Cache data for faster access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview of Workshop\n",
    "- **MapReduce:** What and Why\n",
    "    - Examples:\n",
    "        - Word Count\n",
    "        - Analyzing Status Updates\n",
    "        - Matrix Multiplier\n",
    "    - References and Resources\n",
    "- **Hadoop:** What and Why\n",
    "- **(Py)Spark:** What and Why\n",
    "    - Examples:\n",
    "        - Word Count\n",
    "        - Logistic Regression\n",
    "        - Clickstream\n",
    "    - References and Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MapReduce: What and Why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is MapReduce?\n",
    "\n",
    "(Material in the slides on MapReduce was taken from Chapter 24: MapReduce in \"Data Science from Scratch\" by J. Grus)\n",
    "\n",
    "Basic version of `MapReduce` algorithm:\n",
    "1. User a `mapper` function to turn each item into zero or more key-value\n",
    "pairs.\n",
    "2. Collect together all the pairs with identical keys.\n",
    "3. User a `reducer` function on each collection of grouped values to produce output values for the corresponding key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why MapReduce?\n",
    "\n",
    "\"The primary benefit of MapReduce is that it allows us to distribute computations by moving the processing to the data.\"\n",
    "-- J. Grus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tokenize import tokenize\n",
    "\n",
    "def word_count_old(documents):\n",
    "    \"\"\"word count not using MapReduce\"\"\"\n",
    "    return Counter(word\n",
    "        for document in documents\n",
    "        for word in tokenize(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wc_mapper(document):\n",
    "    \"\"\"for each word in document, emit (word,1)\"\"\"\n",
    "    for word in tokenize(document):\n",
    "        yield (word, 1)\n",
    "        \n",
    "def wc_reducer(word, counts):\n",
    "    \"\"\"sum up the counts for a word\"\"\"\n",
    "    yield (word, sum(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def word_count(documents):\n",
    "    \"\"\"count the words in the input documents using MapReduce\"\"\"\n",
    "    \n",
    "    # place to store grouped values\n",
    "    collector = defaultdict(list)\n",
    "    \n",
    "    for document in documents:\n",
    "        for word, count in wc_mapper(document):\n",
    "            collector[word].append(count)\n",
    "            \n",
    "    return [output\n",
    "            for word, counts in collector.iteritems()\n",
    "            for output in wc_reducer(word, counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_reduce(inputs, mapper, reducer):\n",
    "    \"\"\"runs MapReduce on the inputs using mapper and reducer\"\"\"\n",
    "    collector = defaultdict(list)\n",
    "    \n",
    "    for input in inputs:\n",
    "        for key, value in mapper(input):\n",
    "            collector[key].append(value)\n",
    "            \n",
    "    return [output\n",
    "           for key, values in collector.iteritems()\n",
    "           for output in reducer(key, values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace this with input file\n",
    "documents = ['foo', 'bar', 'baz', 'data', 'science']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_counts = map_reduce(documents, wc_mapper, wc_reducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Analyzing Status Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Matrix Multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Resources and References (MapReduce)\n",
    "- \"Data Science From Scratch\" - J. Grus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### For Futher Exploration\n",
    "- The most widely used MapReduce system is Hadoop (http://hadoop.apache.org), which itself merits many books.  There are various commercial and noncommercial distributions and a huge ecosystem of Hadoop-related tools.\n",
    "    In order to use it, you have to set up your own *cluster* (or find someone to let you use theirs), which is not necessarily a task for the faint-hearted.  Hadoop mappers and reducers are commonly written in Java, although there is a facility known as \"Hadoop streaming\" that allows you to write them in other languages (including Python).\n",
    "\n",
    "- Amazon.com offers an Elastic MapReduce (http://aws.amazon.com/elasticmapreduce/) service that can programmatically create and destroy clusters, charging only for the amount of time that you're using them.\n",
    "\n",
    "- mrjob (https://github.com/Yelp/mrjob) is a Python package for interfacing with Hadoop (or Elastic MapReduce).\n",
    "\n",
    "- Hadoop jobs are typically high-latency, which makes them a poor choice for \"real-time\" analytics.  There are various \"real-time\" tools built on top of Hadoop, but there are also several alternative frameworks that are growing in popularity.  Two of the most popular are Spark (http://spark.apache.org/) and Storm (http://storm.incubator.apache.org/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hadoop: What and Why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is Hadoop?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why Hadoop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: MapReduce for Hadoop Streaming\n",
    "\n",
    "https://github.com/jddavis-100/Statistics-and-Machine-Learning/blob/gh-pages/MapReduce_Word_Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code is an outline/example of how to write a map reduce program in python.\n",
    "This is helpful for writing a simple MapReduce program for streaming on Hadoop. \n",
    "For more information on MapReduce and Hadoop Streaming,\n",
    "see the wiki page titled Hadoop Streaming and MapReduce. Other scripts are uploaded sepparately.\n",
    "General references to learn about Hadoop streaming and python MapReduce jobs will be listed on the wiki.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "#input comes from STDIN (standard input)\n",
    "#feeding in lines and then counting words\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        print('{}\\t{}'.format(word, 1)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count =0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    \n",
    "    #parse the input we got form mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "    \n",
    "    #convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        #count was not a number, so silently\n",
    "        #ignore/discard this line\n",
    "        continue\n",
    "    \n",
    "    #this IF-switch only works because Hadoop sorts map output\n",
    "    #by key (here:word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            #write result to STDOUT\n",
    "            print('{}\\t{}'.format(current_word, current_count))\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "  \n",
    "#do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "     print('{}\\t{}'.format(current_word, current_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark (PySpark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Environment Setup\n",
    "We'll use the free Databricks Comunity Edition platform to run our Spark jobs: \n",
    "1. Use Google Chrome browser (Firefox should also work, but not Internet Explorer, Safari, etc.)\n",
    "2. Sign up for the Community Edition account here: https://databricks.com/try-databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Or, feel free to use a local installation of Spark, etc. If Spark isn't already installed on your machine it can take up to an hour to download and build from source locally (there are also pre-built versions that would be faster to set up):\n",
    "1. Download: http://spark.apache.org/downloads.html\n",
    "2. Open: `../spark1.6.1/README.md`\n",
    "3. Build: `../spark1.6.1/build/mvn -DskipTests clean package`\n",
    "    - On my laptop, time to build was ~30 mins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark: What and Why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is Spark?\n",
    "Spark is a fast and expressive cluster computing system for doing Big Data computation. It's good for iterative tasks, for doing big batch processing, and for interactive data exploration.\n",
    "\n",
    "It's compatible with Hadoop-supported file systems and data formats (HDFS, S3, SequenceFile, ...), so if you've been using Hadoop you can use it with your existing data and deploy it on your existing clusters.\n",
    "\n",
    "It achieves fault tolerance through *lineage*: if you lose a partition (chunk) of data you can reconstruct it through a set of *transformations* that act on data stored in memory. This is in contrast to distributed shared memory systems where you have to write to disk and roll back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why use Spark?\n",
    "- Speed\n",
    "- Ease of Use\n",
    "- Generality\n",
    "- Runs Everywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    ">\"Although current frameworks provide numerous abstractions for accessing a cluster’s computational resources, they lack abstractions for leveraging distributed memory. This makes them inefficient for an important class of emerging applications: those that reuse intermediate results across multiple computations. Data reuse is common in many <font color='blue'>iterative</font> machine learning and graph algorithms, including PageRank, K-means clustering, and logistic regression. Another compelling use case is <font color='blue'>interactive</font> data mining, where a user runs multiple ad-hoc queries on the same subset of the data. Unfortunately, in most current frameworks, the only way to reuse data between computations (e.g., between two MapReduce jobs) is to write it to an external stable storage system, e.g., a distributed file system. This incurs substantial overheads due to data replication, disk I/O, and serialization, which can dominate application execution times.\"\n",
    "\n",
    "- Zaharia et al., \"Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing,\" *In NSDI '12*, April 2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark vs MapReduce vs MPI vs ...\n",
    "- [MapReduce](https://en.wikipedia.org/wiki/MapReduce) --> [Hadoop](http://hadoop.apache.org/): heavily used in business computing\n",
    "- [Message Passing Interface (MPI)](https://en.wikipedia.org/wiki/Message_Passing_Interface) --> [MVAPICH](http://mvapich.cse.ohio-state.edu/): heavily used in scientific computing\n",
    "- [Spark](http://spark.apache.org/): complement to Hadoop, faster for iterative applications, rich set of APIs in Scala, Python, and Java, and an interactive shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark Architecture\n",
    "- Spark Driver and Workers\n",
    "- SparkContext (replaced by SparkSession in version 2.X)\n",
    "- Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program).\n",
    "- SparkContext can connect to several types of cluster managers (either Spark’s own standalone cluster manager, Mesos or YARN)\n",
    "\n",
    "<img src=images/cluster-overview.png align=\"center\" width=\"75%\">\n",
    "\n",
    "<h4 align='right'>https://spark.apache.org/docs/1.1.0/cluster-overview.html</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark Programming Concepts (version 1.X)\n",
    "- ****SparkContext****: entry point to Spark functions\n",
    "- ****Resilient Distributed Datasets (RDDs)****:\n",
    "    - Immutable, distributed collections of objects\n",
    "    - Can be cached in memory for fast reuse\n",
    "- ****Operations on RDDs****:\n",
    "    - *Transformations*: define a new RDD (map, join, ...)\n",
    "    - *Actions*: return or output a result (count, save, ...)\n",
    "- ****Two ways to create RDDs****:\n",
    "    1. By parallelizing an existing collection in your driver program:  \n",
    "        `data = [1, 2, 3, 4, 5]  \n",
    "        distData = sc.parallelize(data)`  \n",
    "    2. Or by referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat:  \n",
    "        `distFile = sc.textFile(\"data.txt\")`       \n",
    "<h4 align='right'>http://spark.apache.org/docs/latest/programming-guide.html</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark Data Interfaces (versions 1.X and 2.X)\n",
    "\n",
    "There are several key interfaces that you should understand when you go to use Spark.\n",
    "\n",
    "-   ****The Dataset****\n",
    "    -   The Dataset is Apache Spark's newest distributed collection and can be considered a combination of DataFrames and RDDs. It provides the typed interface that is available in RDDs while providing a lot of conveniences of DataFrames. It will be the core abstraction going forward.\n",
    "-   ****The DataFrame****\n",
    "    -   The DataFrame is collection of distributed `Row` types. These provide a flexible interface and are similar in concept to the DataFrames you may be familiar with in python (pandas) as well as in the R language.\n",
    "-   ****The RDD (Resilient Distributed Dataset)****\n",
    "    -   Apache Spark's first abstraction was the RDD or Resilient Distributed Dataset. Essentially it is an interface to a sequence of data objects that consist of one or more types that are located across a variety of machines in a cluster. RDD's can be created in a variety of ways and are the \"lowest level\" API available to the user. While this is the original data structure made available, new users should focus on Datasets as those will be supersets of the current RDD functionality.\n",
    "\n",
    "*(slide taken from \"Introduction to Apache Spark on Databricks\" notebook)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is PySpark?\n",
    "- The Python API for Spark\n",
    "- Run interactive jobs in the shell\n",
    "- Supports numpy, pandas and other Python libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Why use PySpark?\n",
    "- If you already know Python\n",
    "- Can use Spark in tandem with your favorite Python libraries\n",
    "- If you don't need Python libraries, maybe just write code in Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PySpark's core classes (version 1.X):\n",
    "- ****pyspark.SparkContext****  \n",
    "Main entry point for Spark functionality.\n",
    "- ****pyspark.RDD****  \n",
    "A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n",
    "- ****pyspark.streaming.StreamingContext****  \n",
    "Main entry point for Spark Streaming functionality.\n",
    "- ****pyspark.streaming.DStream****  \n",
    "A Discretized Stream (DStream), the basic abstraction in Spark Streaming.\n",
    "- ****pyspark.sql.SQLContext****  \n",
    "Main entry point for DataFrame and SQL functionality.\n",
    "- ****pyspark.sql.DataFrame****  \n",
    "A distributed collection of data grouped into named columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transformations\n",
    "- Transform one RDD to another, **new** RDD (immutable)\n",
    "\n",
    "| Transformation | Description | Type |\n",
    "| :------:  | :-----------: | :-----: |\n",
    "| `map(func)`     | Apply a function over each element | Narrow |\n",
    "| `flatMap(func)` | Map then flatten output | Narrow |\n",
    "| `filter(func)`  | Keep only elements where function is `True` | Narrow |\n",
    "| `sample(withReplacement, fraction, seed)` | Return a sampled subset of this RDD | Narrow |\n",
    "| `groupByKey(k, v)` | Group the values for each key in the RDD into a single sequence | Wide |\n",
    "| `reduceByKey(func)` | Merge the values for each key using an associative reduce function | Wide |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=images/narrow_wide_transformations.png align=\"center\" width=\"100%\">\n",
    "\n",
    "<h4 align='right'>https://dzone.com/articles/big-data-processing-spark</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Actions\n",
    "- Return or output a result\n",
    "\n",
    "| Action | Description | Try it Out\\*|\n",
    "| :------:  | :-----------:| :---: |\n",
    "| `collect()`     | Return a list that contains all of the elements in this RDD | `sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()` |\n",
    "| `count()`  | Return the number of elements | `sc.parallelize([2, 3, 4]).count()` |\n",
    "| `saveAsTextFile(path)` | Save as a text file, using string representations of elements | `sc.parallelize(['foo', '-', 'bar', '!']).saveAsTextFile(\"/FileStore/foo-bar.txt\")])`|\n",
    "| `first()`    | Return the first element | `sc.parallelize([2, 3, 4]).first()` |\n",
    "| `take(num)`    | Take the first num elements | `sc.parallelize([2, 3, 4, 5, 6]).take(2)` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### \\* Try it Out:\n",
    "1. Go to your databricks Workspace and create a new directory within your Users directory called \"2016-06-20-pyladies-pyspark\" \n",
    "2. Create a notebook called \"0-Introduction\"  within this directory\n",
    "3. Type or copy/paste lines of code into separate cells and run them (you will be prompted to launch a cluster) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When using Databricks the `SparkContext` is created for you automatically as `sc`.\n",
    "\n",
    "In the Databricks Community Edition there are no Worker Nodes - the Driver Program (Master) executes the entire code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Try a couple more examples with transformations and actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.groupByKey().mapValues(len).collect())\n",
    "sorted(rdd.groupByKey().mapValues(list).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.reduceByKey(add).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've shown only a subset of possible *transformations* and *actions*. Check out others for your application in the docs: http://spark.apache.org/docs/latest/api/python/pyspark.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: Log Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "val lines = spark.textFile(\"hdfs://...\")\n",
    "val errors = lines.filter(_.startsWith(\"ERROR\"))\n",
    "val messages = errors.map(_.split('\\t')(2))\n",
    "\n",
    "messages.filter(_.contains(\"foo\")).count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The computation is expressed declaratively and nothing actually takes place until calling `count` at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise 1: Word Count\n",
    "Create a few transformations to build a dataset of (String, Int) pairs called counts and then save it to a file.\n",
    "\n",
    "1. Create a notebook in \"2016-06-20-pyladies-pyspark\" called \"1-WordCount\"\n",
    "2. Try to implement the following Word Count example:\n",
    "\n",
    "http://spark.apache.org/examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text_file = sc.textFile(\"hdfs://...\")\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "counts.saveAsTextFile(\"hdfs://...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise 2: Logistic Regression\n",
    "\n",
    "1. Create a notebook in \"2016-06-20-pyladies-pyspark\" called \"2-LogisticRegression\"\n",
    "2. Try to implement one of the following Logistic Regression examples:\n",
    "    - http://spark.apache.org/examples.html (Prediction with Logistic Regression)\n",
    "    - https://github.com/apache/spark/blob/master/examples/src/main/python/mllib/logistic_regression.py\n",
    "    - https://github.com/apache/spark/blob/master/examples/src/main/python/logistic_regression.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Every record of this DataFrame contains the label and\n",
    "# features represented by a vector.\n",
    "df = sqlContext.createDataFrame(data, [\"label\", \"features\"])\n",
    "\n",
    "# Set parameters for the algorithm.\n",
    "# Here, we limit the number of iterations to 10.\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "# Fit the model to the data.\n",
    "model = lr.fit(df)\n",
    "\n",
    "# Given a dataset, predict each point's label, and show the results.\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercise 3: Clickstream\n",
    "\n",
    "1. Create a notebook in \"2016-06-20-pyladies-pyspark\" called \"3-Clickstream\"\n",
    "2. Implement the Clickstream example from the Databricks \"Quick Start DataFrames\" notebook in Python\n",
    "3. Or just run through the example in Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Resources and References (Spark)\n",
    "- ****MOOCs****:\n",
    "    - \"Introduction to Apache Spark\": https://www.edx.org/course/introduction-apache-spark-uc-berkeleyx-cs105x\n",
    "    - \"Hadoop Platform and Application Framework\" (week 5 covers Spark): https://www.coursera.org/learn/hadoop/home/week/5\n",
    "- ****Spark/PySpark Docs****:\n",
    "    - (v1.6.1 - latest) http://spark.apache.org/docs/latest/\n",
    "        - http://spark.apache.org/docs/latest/api/python/\n",
    "    - (v2.0.0) http://spark.apache.org/docs/2.0.0-preview/\n",
    "    - http://spark.apache.org/research.html\n",
    "    - http://spark.apache.org/examples.html\n",
    "- ****Other****:\n",
    "    - Spark 2.0 Webinar (2016): http://go.databricks.com/apache-spark-2.0-presented-by-databricks-co-founder-reynold-xin\n",
    "    - PySpark Talk (J. Rosen, 2013): https://www.youtube.com/watch?v=xc7Lc8RA8wE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Thanks for Coming!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
